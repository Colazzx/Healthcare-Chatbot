{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import the Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import CTransformers\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load PDF File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF\n",
    "loader = DirectoryLoader(\"Data/\", glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split Text into Chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "text_chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Embeddings and Vector Database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device':\"cpu\"}\n",
    ")\n",
    "\n",
    "# Create vectorstore\n",
    "vector_store = FAISS.from_documents(text_chunks,embeddings)\n",
    "\n",
    "# Setting the retriever\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\":2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Prompt for Chatbot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for prompt\n",
    "def chatbot_prompt():\n",
    "    # Define the prompt template\n",
    "    general_system_template = r\"\"\"\n",
    "    You are an assistant designed to answer questions based on healthcare from users.\n",
    "\n",
    "    **Layer 1: Contextual Understanding**\n",
    "    Please read the following user reviews carefully and provide precise answers to the questions based on the source document context provided. Do not answer any questions outside this context.\n",
    "\n",
    "    **Layer 2: Response Guidelines**\n",
    "    If the answer is not explicitly found in the source document context, kindly state: \"I'm sorry, I don't have that information.\" Please do not fabricate any answers or discuss unrelated topics.\n",
    "    ----\n",
    "    {context}\n",
    "    ----\n",
    "    \"\"\"\n",
    "    \n",
    "    general_user_template = \"Question:```{question}```\"\n",
    "    \n",
    "    messages = [\n",
    "            SystemMessagePromptTemplate.from_template(general_system_template),\n",
    "            HumanMessagePromptTemplate.from_template(general_user_template)\n",
    "    ]\n",
    "    \n",
    "    # Create the PromptTemplate\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "    \n",
    "    return qa_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_chain():\n",
    "    llm = CTransformers(\n",
    "        model=\"llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "        model_type=\"llama\",\n",
    "        config={'max_new_tokens':128,'temperature':0.01}\n",
    "    )\n",
    "\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\", \n",
    "        return_messages=True\n",
    "    )\n",
    "\n",
    "    chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,chain_type='stuff',\n",
    "        retriever=retriever,\n",
    "        memory=memory,\n",
    "        return_source_documents=True,\n",
    "        combine_docs_chain_kwargs={'prompt': chatbot_prompt()}\n",
    "    )\n",
    "    \n",
    "    return chain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
